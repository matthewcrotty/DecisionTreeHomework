{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('adult.data')\n",
    "lines = file.readlines()\n",
    "degree = {\n",
    "    'Doctorate': 15.,\n",
    "    'Masters': 14. ,\n",
    "    'Bachelors': 13.,\n",
    "    'Some-college': 12.,\n",
    "    'Prof-school': 11.,\n",
    "    'Assoc-acdm': 10.,\n",
    "    'Assoc-voc': 9.,\n",
    "    'HS-grad': 8.,\n",
    "    '12th': 7.,\n",
    "    '11th': 6.,\n",
    "    '10th': 5.,\n",
    "    '9th': 4.,\n",
    "    '7th-8th': 3.,\n",
    "    '5th-6th': 2.,\n",
    "    '1st-4th': 1.,\n",
    "    'Preschool': 0.,\n",
    "}\n",
    "occupation = {\n",
    "    'Tech-support':0., \n",
    "    'Craft-repair':1., \n",
    "    'Other-service':2., \n",
    "    'Sales':3., \n",
    "    'Exec-managerial':4., \n",
    "    'Prof-specialty':5., \n",
    "    'Handlers-cleaners':6., \n",
    "    'Machine-op-inspct':7., \n",
    "    'Adm-clerical':8., \n",
    "    'Farming-fishing':9., \n",
    "    'Transport-moving':10, \n",
    "    'Priv-house-serv':11., \n",
    "    'Protective-serv':12., \n",
    "    'Armed-Forces':13.,\n",
    "}\n",
    "race = {\n",
    "    'White': 0.0,\n",
    "    'Asian-Pac-Islander': 1.0,\n",
    "    'Amer-Indian-Eskimo': 2.0,\n",
    "    'Other':3.0,\n",
    "    'Black':4.0\n",
    "}\n",
    "X, Y = [], []\n",
    "for i in range(len(lines)-1):\n",
    "    if '?' in lines[i]:\n",
    "        continue\n",
    "    l = lines[i].strip().split(',')\n",
    "    X.append([float(l[0]), float(l[12]), float(l[4]), degree[l[3].strip()], occupation[l[6].strip()], race[l[8].strip()]])\n",
    "    if l[-1] == ' <=50K':\n",
    "        Y.append(-1.0)\n",
    "    else:\n",
    "        Y.append(1.0)\n",
    "x_test, x_train = X[0:int(0.2*len(X))], X[int(0.2*len(X))+1:]\n",
    "y_test, y_train = Y[0:int(0.2*len(Y))], Y[int(0.2*len(Y))+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7491710875331565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Default model with set random state\n",
    "clf = DecisionTreeClassifier(random_state = 44 )\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Model Accuracy:\", clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7141909814323607\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state = 44, class_weight='balanced')\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Model Accuracy:\", clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7907824933687002\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state = 44, max_leaf_nodes=100)\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Model Accuracy:\", clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7828249336870027\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state = 44, max_depth=10)\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Model Accuracy:\", clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7834880636604774\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state = 44, min_impurity_decrease=0.001)\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Model Accuracy:\", clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By modifying most of the default parameters, I was able to increase the accuracy of the model. The class_weight describes how each class is weighted in the output, with a default with both being equal weights. Because there are only two classes in the data, it doesn't make sense to differentiate between them. However with the other parameters I saw in increase in the accuracy from the default model from 4.5 - 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7934350132625995\n",
      "10-Cross Validation Score: 0.7824937124531866\n"
     ]
    }
   ],
   "source": [
    "# Bagging Decision Trees implementation\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np \n",
    "\n",
    "clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_leaf_nodes=100), n_estimators=10, random_state=44)\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Model Accuracy:\", clf.score(x_test, y_test))\n",
    "val = cross_val_score(clf, x_test, y_test, cv=10)\n",
    "print(\"10-Cross Validation Score:\", np.average(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7854774535809018\n",
      "10-Cross Validation Score: 0.782660648193909\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost implementation\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=10, random_state=44, algorithm='SAMME')\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Model Accuracy:\", clf.score(x_test, y_test))\n",
    "val = cross_val_score(clf, x_test, y_test, cv=10)\n",
    "print(\"10-Cross Validation Score:\", np.average(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Effectiveness\n",
    "\n",
    "In terms of accuracy the Bagging Decision Tree model performed the best when it used the regular DecisionTreeClassifier with max_leaf_nodes=100. It makes sense that this is the case, because the bagging method is supposed to be an improvement over the base models.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "The metric I am using to determine the model effectiveness is the sklearn method score(), which returns the mean accuracy for a given model on the input data. The formula for accuracy is $A = \\frac{Correct Predictions}{Total Predictions}$\n",
    "\n",
    "I'm using the accuracy metric since it is generally accepted to be a reasonable measure of correctness for models. There are other metrics that are similar such as precision, which only looks at the correctness of a certain class, or recall, which looks into the correctness of a certain class and the misses. These metrics can be more valuble than accuracy in certain situations in which the risks of false positives or negatives needs to be considered. Since this problem doesn't neccesarily have a immediate use or risk when failure occurs, accuracy should be a good metric. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
